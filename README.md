# âœˆï¸ Aircraft Flight Data Streaming Pipeline

> Real-time processing of millions of aviation events from thousands of aircraft using Databricks declarative pipelines

[![Databricks](https://img.shields.io/badge/Databricks-DLT-FF3621?logo=databricks)](https://www.databricks.com/)
[![OpenSky Network](https://img.shields.io/badge/Data%20Source-OpenSky%20Network-blue)](https://opensky-network.org/)

## ğŸ¯ Overview

This project demonstrates an end-to-end streaming data pipeline on Databricks that ingests live avionics data from the OpenSky Network API and delivers it to external systems using custom streaming sources and sinks.

## âœ¨ Features

- ğŸ”„ **Real-time ingestion** of flight state vectors from OpenSky Network API
- ğŸ“Š **Managed streaming tables** using Databricks declarative pipelines
- ğŸš€ **Custom streaming source** implementation with PySpark
- ğŸŒ **HTTP sink integration** for external system delivery
- ğŸ“ˆ **Scalable processing** of millions of events

## ğŸ—ï¸ Architecture

OpenSky API â†’ Custom Source â†’ Streaming Table â†’ Append Flow â†’ HTTP Sink â†’ External Endpoint

**Components:**
- Custom Spark streaming source
- Declarative pipeline streaming table: `opensky_flights`
- Append flow with `dlt.read_stream`
- Custom HTTP POST sink (RequestBin/Pipedream)

## ğŸ“¦ Prerequisites

- âœ… Databricks workspace with Declarative Pipelines support
- âœ… Network access to:
  - [OpenSky Network API](https://opensky-network.org/api/states/all)
  - [RequestBin/Pipedream](https://pipedream.com/requestbin) endpoint
- âœ… Cluster runtime supporting Python Data Source APIs
- âœ… RequestBin or Pipedream account for testing

## ğŸš€ Getting Started

### Step 1: Navigate to Explorations

- ğŸ‘‰ Open the explorations folder to access the implementation notebooks

### Step 2: Configure Your Environment
- Set up your RequestBin/Pipedream HTTPS endpoint
- Note your endpoint URL for configuration

### Step 3: Run the Pipeline
Notebook 1: Source & Table Setup
- Register the OpenSky custom streaming source
- Create the opensky_flights streaming table
- Start the pipeline

Notebook 2: Sink Configuration
- Register the RequestBin custom sink writer
- Configure sink with your endpoint and batch size
- Start the append flow

### Step 4: Validate
- Monitor the lakehouse table in Databricks
- Check received payloads in your RequestBin/Pipedream dashboard

ğŸ“ License
- This project is available for educational and demonstration purposes.

ğŸ¤ Contributing
- Contributions, issues, and feature requests are welcome!

## Ready to process millions of flight events? Head to the explorations folder and get started! ğŸš€
